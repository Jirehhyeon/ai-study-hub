# LangChain ê¸°ì´ˆ - LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì˜ ì‹œì‘

## ğŸ”— LangChainì´ë€?

LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

### í•µì‹¬ ê°œë…
- **ğŸ”— Chains**: ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì‘ì—… ìˆ˜í–‰
- **ğŸ“ Prompts**: íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ë° ìµœì í™”
- **ğŸ§  Memory**: ëŒ€í™” ë§¥ë½ ìœ ì§€ ë° ìƒíƒœ ê´€ë¦¬
- **ğŸ” Retrieval**: RAGë¥¼ ìœ„í•œ ë¬¸ì„œ ê²€ìƒ‰ ë° ì²˜ë¦¬
- **ğŸ¤– Agents**: ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ììœ¨ ì—ì´ì „íŠ¸

---

## 1. ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

```bash
# LangChain ì„¤ì¹˜
pip install langchain langchain-openai langchain-community

# ì¶”ê°€ íŒ¨í‚¤ì§€
pip install chromadb  # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
pip install tiktoken  # í† í° ì¹´ìš´íŒ…
pip install python-dotenv  # í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```python
# .env íŒŒì¼ ìƒì„±
"""
OPENAI_API_KEY=your_api_key_here
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_key
"""

# Pythonì—ì„œ ë¡œë“œ
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
```

---

## 2. ê¸°ë³¸ ì‚¬ìš©ë²•

### LLM ì´ˆê¸°í™”

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

# LLM ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=1000
)

# ê°„ë‹¨í•œ ëŒ€í™”
response = llm.invoke([
    SystemMessage(content="ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
    HumanMessage(content="LangChainì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
])
print(response.content)
```

### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ {role}ì…ë‹ˆë‹¤. {style}í•˜ê²Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”."),
    ("user", "{input}")
])

# ì²´ì¸ ìƒì„±
chain = prompt | llm

# ì‹¤í–‰
result = chain.invoke({
    "role": "Python ì „ë¬¸ê°€",
    "style": "ì¹œì ˆí•˜ê³  ìƒì„¸",
    "input": "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
})
print(result.content)
```

---

## 3. ì²´ì¸(Chains) í™œìš©í•˜ê¸°

### ê¸°ë³¸ ì²´ì¸

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt_template = PromptTemplate(
    input_variables=["topic"],
    template="""
    ì£¼ì œ: {topic}
    
    ìœ„ ì£¼ì œì— ëŒ€í•´ ë‹¤ìŒì„ í¬í•¨í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”:
    1. ì •ì˜
    2. ì£¼ìš” íŠ¹ì§•
    3. ì‹¤ì œ í™œìš© ì˜ˆì‹œ
    """
)

# ì²´ì¸ ìƒì„±
chain = LLMChain(llm=llm, prompt=prompt_template)

# ì‹¤í–‰
result = chain.run(topic="ë¨¸ì‹ ëŸ¬ë‹")
print(result)
```

### ìˆœì°¨ì  ì²´ì¸

```python
from langchain.chains import SimpleSequentialChain

# ì²« ë²ˆì§¸ ì²´ì¸: ì£¼ì œì— ëŒ€í•œ ì„¤ëª… ìƒì„±
explain_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["topic"],
        template="ë‹¤ìŒ ì£¼ì œë¥¼ ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”: {topic}"
    )
)

# ë‘ ë²ˆì§¸ ì²´ì¸: ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ í€´ì¦ˆ ìƒì„±
quiz_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["explanation"],
        template="ë‹¤ìŒ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ 3ê°œì˜ í€´ì¦ˆë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”:\n\n{explanation}"
    )
)

# ìˆœì°¨ì  ì²´ì¸ ìƒì„±
sequential_chain = SimpleSequentialChain(
    chains=[explain_chain, quiz_chain],
    verbose=True
)

# ì‹¤í–‰
result = sequential_chain.run("ë”¥ëŸ¬ë‹")
print(result)
```

---

## 4. ë©”ëª¨ë¦¬(Memory) ê´€ë¦¬

### ëŒ€í™” ë©”ëª¨ë¦¬

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
memory = ConversationBufferMemory()

# ëŒ€í™” ì²´ì¸ ìƒì„±
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# ëŒ€í™” ì§„í–‰
print(conversation.predict(input="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AIë¥¼ ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤."))
print(conversation.predict(input="íŠ¹íˆ ìì—°ì–´ì²˜ë¦¬ì— ê´€ì‹¬ì´ ë§ì•„ìš”."))
print(conversation.predict(input="ì œê°€ ë­˜ ê³µë¶€í•œë‹¤ê³  í–ˆì£ ?"))  # ì´ì „ ëŒ€í™” ê¸°ì–µ
```

### ìš”ì•½ ë©”ëª¨ë¦¬

```python
from langchain.memory import ConversationSummaryMemory

# ìš”ì•½ ë©”ëª¨ë¦¬ (ê¸´ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì—¬ ì €ì¥)
summary_memory = ConversationSummaryMemory(llm=llm)

conversation_with_summary = ConversationChain(
    llm=llm,
    memory=summary_memory,
    verbose=True
)

# ê¸´ ëŒ€í™”ë¥¼ ì§„í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ìš”ì•½ë¨
for i in range(5):
    response = conversation_with_summary.predict(
        input=f"ë¨¸ì‹ ëŸ¬ë‹ì˜ {i+1}ë²ˆì§¸ ì¤‘ìš”í•œ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    )
    print(f"Turn {i+1}: {response[:100]}...")
```

---

## 5. ë¬¸ì„œ ë¡œë”ì™€ í…ìŠ¤íŠ¸ ë¶„í• 

### ë¬¸ì„œ ë¡œë”©

```python
from langchain.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
# loader = TextLoader("example.txt", encoding="utf-8")
# documents = loader.load()

# PDF íŒŒì¼ ë¡œë“œ (ì˜ˆì‹œ)
# pdf_loader = PyPDFLoader("example.pdf")
# pdf_documents = pdf_loader.load()

# ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±
sample_text = """
ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ë¡œ, ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.
ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ì¢…ë¥˜ë¡œ, ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
ìì—°ì–´ì²˜ë¦¬(NLP)ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
"""

# í…ìŠ¤íŠ¸ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=50,
    chunk_overlap=10,
    length_function=len,
)

chunks = text_splitter.split_text(sample_text)
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}: {chunk}")
```

---

## 6. ë²¡í„° ìŠ¤í† ì–´ì™€ ê²€ìƒ‰

### ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.schema import Document

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
embeddings = OpenAIEmbeddings()

# ë¬¸ì„œ ì¤€ë¹„
documents = [
    Document(page_content="Pythonì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.", metadata={"topic": "programming"}),
    Document(page_content="ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.", metadata={"topic": "ml"}),
    Document(page_content="ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì…ë‹ˆë‹¤.", metadata={"topic": "dl"}),
    Document(page_content="ìì—°ì–´ì²˜ë¦¬ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.", metadata={"topic": "nlp"}),
]

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
vector_store = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# ìœ ì‚¬ë„ ê²€ìƒ‰
query = "ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” AI ê¸°ìˆ "
results = vector_store.similarity_search(query, k=2)

for i, doc in enumerate(results):
    print(f"Result {i+1}: {doc.page_content}")
    print(f"Metadata: {doc.metadata}\n")
```

---

## 7. RAG (Retrieval-Augmented Generation) êµ¬í˜„

```python
from langchain.chains import RetrievalQA

# QA ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vector_store.as_retriever(search_kwargs={"k": 2}),
    return_source_documents=True
)

# ì§ˆë¬¸í•˜ê¸°
question = "ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê´€ê³„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
result = qa_chain({"query": question})

print(f"ì§ˆë¬¸: {question}")
print(f"ë‹µë³€: {result['result']}")
print("\nì°¸ì¡°í•œ ë¬¸ì„œ:")
for doc in result['source_documents']:
    print(f"- {doc.page_content}")
```

---

## 8. ì—ì´ì „íŠ¸(Agents) í™œìš©

### ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸

```python
from langchain.agents import create_react_agent, AgentExecutor
from langchain.tools import Tool
from langchain import hub

# ê°„ë‹¨í•œ ë„êµ¬ ì •ì˜
def calculate(expression: str) -> str:
    """ìˆ˜í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        result = eval(expression)
        return f"ê³„ì‚° ê²°ê³¼: {result}"
    except:
        return "ê³„ì‚°í•  ìˆ˜ ì—†ëŠ” í‘œí˜„ì‹ì…ë‹ˆë‹¤."

def get_current_time() -> str:
    """í˜„ì¬ ì‹œê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    from datetime import datetime
    return f"í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

# ë„êµ¬ ë¦¬ìŠ¤íŠ¸
tools = [
    Tool(
        name="Calculator",
        func=calculate,
        description="ìˆ˜í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì…ë ¥: ìˆ˜ì‹"
    ),
    Tool(
        name="Clock",
        func=get_current_time,
        description="í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì¤ë‹ˆë‹¤."
    )
]

# ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸ (ReAct ë°©ì‹)
prompt = hub.pull("hwchase17/react")

# ì—ì´ì „íŠ¸ ìƒì„±
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# ì—ì´ì „íŠ¸ ì‹¤í–‰
response = agent_executor.invoke({
    "input": "í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì£¼ê³ , 3ì‹œê°„ 45ë¶„ í›„ëŠ” ëª‡ ì‹œì¸ì§€ ê³„ì‚°í•´ì£¼ì„¸ìš”."
})
print(response["output"])
```

---

## 9. ì‹¤ì „ í”„ë¡œì íŠ¸: PDF ë¬¸ì„œ ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ

```python
# ì „ì²´ ì‹œìŠ¤í…œ êµ¬í˜„
class PDFQuestionAnswering:
    def __init__(self, pdf_path=None):
        self.llm = ChatOpenAI(temperature=0)
        self.embeddings = OpenAIEmbeddings()
        self.vector_store = None
        
    def load_pdf(self, pdf_path):
        """PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        # ì‹¤ì œë¡œëŠ” PyPDFLoaderë¥¼ ì‚¬ìš©
        # loader = PyPDFLoader(pdf_path)
        # documents = loader.load()
        
        # ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´
        documents = [
            Document(page_content="LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."),
            Document(page_content="ì²´ì¸ì„ í†µí•´ ì—¬ëŸ¬ ì‘ì—…ì„ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
            Document(page_content="RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ì…ë‹ˆë‹¤."),
        ]
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        self.vector_store = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings
        )
        
    def ask_question(self, question):
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.vector_store:
            return "ë¨¼ì € ë¬¸ì„œë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        # QA ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(),
            return_source_documents=True
        )
        
        # ì§ˆë¬¸ ì²˜ë¦¬
        result = qa_chain({"query": question})
        return result['result']

# ì‹œìŠ¤í…œ ì‚¬ìš©
qa_system = PDFQuestionAnswering()
qa_system.load_pdf("dummy_path.pdf")  # ì‹¤ì œë¡œëŠ” PDF ê²½ë¡œ

# ì§ˆë¬¸í•˜ê¸°
questions = [
    "LangChainì´ ë¬´ì—‡ì¸ê°€ìš”?",
    "ì²´ì¸ì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "RAGë€ ë¬´ì—‡ì¸ê°€ìš”?"
]

for q in questions:
    answer = qa_system.ask_question(q)
    print(f"\nQ: {q}")
    print(f"A: {answer}")
```

---

## 10. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. ì—ëŸ¬ ì²˜ë¦¬
```python
from langchain.callbacks import StdOutCallbackHandler
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def safe_llm_call(chain, input_data):
    try:
        return chain.invoke(input_data)
    except Exception as e:
        print(f"Error: {e}")
        raise
```

### 2. í† í° ê´€ë¦¬
```python
import tiktoken

def count_tokens(text, model="gpt-3.5-turbo"):
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# ì‚¬ìš© ì˜ˆ
text = "ì´ê²ƒì€ í† í° ì¹´ìš´íŒ… ì˜ˆì‹œì…ë‹ˆë‹¤."
token_count = count_tokens(text)
print(f"í† í° ìˆ˜: {token_count}")
```

### 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
```python
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# ìŠ¤íŠ¸ë¦¬ë° LLM
streaming_llm = ChatOpenAI(
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]
)

# ì‹¤ì‹œê°„ ì‘ë‹µ
streaming_llm.invoke("íŒŒì´ì¬ì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
```

---

## ğŸ¯ ì—°ìŠµ ë¬¸ì œ

1. **ì±„íŒ…ë´‡ ë§Œë“¤ê¸°**: ë©”ëª¨ë¦¬ë¥¼ ê°€ì§„ ëŒ€í™”í˜• ì±—ë´‡ êµ¬í˜„
2. **ë¬¸ì„œ ìš”ì•½ê¸°**: ê¸´ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ëŠ” ì²´ì¸ êµ¬í˜„
3. **ë‹¤êµ­ì–´ ë²ˆì—­ê¸°**: ì—¬ëŸ¬ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ìˆœì°¨ ì²´ì¸
4. **ì½”ë“œ ë¦¬ë·°ì–´**: ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ê°œì„ ì ì„ ì œì•ˆí•˜ëŠ” ì—ì´ì „íŠ¸

---

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ

- [LangChain ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/)
- [LangChain ì˜ˆì œ ê°¤ëŸ¬ë¦¬](https://github.com/langchain-ai/langchain/tree/master/cookbook)
- [LangSmith (ë””ë²„ê¹… ë„êµ¬)](https://smith.langchain.com/)
- [LangChain ì»¤ë®¤ë‹ˆí‹°](https://github.com/langchain-ai/langchain/discussions)

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. **ê³ ê¸‰ RAG**: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, ë¦¬ë­í‚¹
2. **í”„ë¡œë•ì…˜ ë°°í¬**: API ì„œë²„ êµ¬ì¶•, ìŠ¤ì¼€ì¼ë§
3. **ê³ ê¸‰ ì—ì´ì „íŠ¸**: ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
4. **ìµœì í™”**: ë¹„ìš© ì ˆê°, ì„±ëŠ¥ í–¥ìƒ ê¸°ë²•

**ì‹¤ìŠµì´ í•µì‹¬ì…ë‹ˆë‹¤! ì‘ì€ í”„ë¡œì íŠ¸ë¶€í„° ì‹œì‘í•´ë³´ì„¸ìš”.** ğŸ”¨